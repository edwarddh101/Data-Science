# Data-Science

Spark
1. Rich API
2. Data Pipeline, locally(python), scale(spark)
3. Faster compare to hadoop, esp. iteration 
4. Spark Execution
5. SparkContext(laptop) -- Cluster Manager(Standalone/YARN/Mesos) -- Executor(Worker Node-->Cluster)
6. Driver--Executor--Master--Worker(Terminology)
7. PySpark: incorp spark library in Python --> Send it to spark worker on Cluster-->feedback to local
8. 
